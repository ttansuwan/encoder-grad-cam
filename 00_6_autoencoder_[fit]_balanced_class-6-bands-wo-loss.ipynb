{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aedd964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import signal, misc\n",
    "from matplotlib import pyplot as plt \n",
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt, iirnotch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import sem\n",
    "from scipy import mean\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from __future__ import print_function\n",
    "from scipy.stats import norm\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Concatenate, AveragePooling2D, BatchNormalization, Conv2D, Conv2DTranspose, Dense, Input, Reshape, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "11518d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "# or \n",
    "GPU = 0\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[GPU], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ffbf1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 800 # Time point in each segment (freq. rate x 4s ec [4 sec/window])\n",
    "C = 3\n",
    "D = 6 # Dim (fs bands)\n",
    "FS = 200 # Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e907ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_data(Folder_name, label=0.):\n",
    "    all_data_segment = glob('../LFP_Bank/'+Folder_name+'/*_200Hz.npy')\n",
    "\n",
    "    data_all = []\n",
    "    for indivi_file_segment in all_data_segment:\n",
    "        print (indivi_file_segment)\n",
    "        load_data = np.load(indivi_file_segment, allow_pickle=True)\n",
    "        X, Y = load_data.shape\n",
    "        N = int(Y/T) # Number of segment(s)\n",
    "        data_new = []\n",
    "        for i in range (N):\n",
    "            data = load_data[:, i*T:((i+1)*T)]\n",
    "            data_new.append(data)\n",
    "        data_output = np.array(data_new)\n",
    "        data_all.append(data_output)\n",
    "        #print('=================================================================')\n",
    "        #print(data_output)\n",
    "        #print('=================================================================')\n",
    "        #print('=============')\n",
    "        #print(data_output.shape)\n",
    "        #print('=============')\n",
    "        #print('==============================================================================================================')\n",
    "    X = np.array(data_all)\n",
    "    y = np.full((X.shape[0], X.shape[1]), label)\n",
    "    return X, y\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "72900c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(margin = 1.0):\n",
    "    def inner_triplet_loss_objective(y_true, y_pred):\n",
    "        labels = y_true\n",
    "        embeddings = y_pred\n",
    "        return tfa.losses.triplet_semihard_loss(y_true = labels, y_pred = embeddings, margin = margin)\n",
    "    return inner_triplet_loss_objective\n",
    "\n",
    "def dummy_loss(margin = 1.0):\n",
    "    def inner(y_true, y_pred):\n",
    "        return 0\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "38ffa55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(num_class, F1=16, F2=8, P=(1, 10)):\n",
    "    'encoder'\n",
    "    encoder_input = Input(shape=(D, T, C))\n",
    "    x = layers.Conv2D(F1, P, activation='elu', padding='same')(encoder_input)\n",
    "    x = BatchNormalization(axis=3, epsilon=1e-05, momentum=0.1)(x)\n",
    "    x = layers.AveragePooling2D(P, padding='same')(x)\n",
    "    x = layers.Conv2D(F2, P, activation='elu', padding='same')(x)\n",
    "    x = BatchNormalization(axis=3, epsilon=1e-05, momentum=0.1)(x)\n",
    "    x = layers.AveragePooling2D(P, padding='same')(x)\n",
    "#     x = layers.Conv2D(8, (1, 50), activation='elu', padding='same')(x)\n",
    "#     x = layers.AveragePooling2D((1, 2), padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    encoder_output = Dense(FS)(x)\n",
    "    ### the end of the layers\n",
    "    encoder        = models.Model(inputs=encoder_input, outputs=encoder_output, name='encoder')\n",
    "    encoder.summary()\n",
    "    \n",
    "    'decoder'\n",
    "    decoder_input = Input(shape=(FS), name='decoder_input')\n",
    "    x = Dense(D*(T//100)*F2, activation='elu')(decoder_input) \n",
    "    x = Reshape((D, T//100, F2))(x)\n",
    "    x = layers.Conv2D(F2, P, activation='elu', padding='same')(x)\n",
    "    x = layers.UpSampling2D(P)(x)\n",
    "    x = layers.Conv2D(F1, P, activation='elu', padding='same')(x)\n",
    "    x = layers.UpSampling2D(P)(x)\n",
    "#     x = layers.Conv2D(16, (1, 10), activation='elu', padding='same')(x)\n",
    "#     x = layers.UpSampling2D((1, 1))(x)\n",
    "    decoder_output = layers.Conv2D(C, P, activation='elu', padding='same')(x)\n",
    "    # the end of the layers \n",
    "    decoder        = models.Model(inputs=decoder_input, outputs=decoder_output, name='decoder')\n",
    "    decoder.summary()\n",
    "    \n",
    "    'Build the computation graph for training'\n",
    "    latent  = encoder(encoder_input)\n",
    "    train_xr= decoder(latent)\n",
    "    z       = Dense(units=num_class, activation='softmax', kernel_constraint=max_norm(0.5), name='classifier')(latent)\n",
    "\n",
    "    return models.Model(inputs = [encoder_input], outputs = [train_xr, z],  name = 'AE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "764ddcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_folders = [\"00_Blank Syrup\", '00_Control group',\"00_PEG+Saline (control)\", '01_PEG+1mg METH', '02_PEG+5mg METH',\n",
    "#                '03_Morphine I (5 mg, ip)', '04_Morphine II (15 mg, ip)', '05_Cannabis project (THC 50 mg, ip)',\n",
    "#                '06_MDMA project (10 mg, ip)', '07_L-DOPA project (25 mg in dw, po)', '08_Ephedrine group (10 mg, ip)',\n",
    "#                '09_Pseudoephedrine group (50 mg, po)', '010_Pseudoephedrine group (100 mg, po)',\n",
    "#                '011_Ketamine project (50 mg, ip)', '012_Lorazepam (1 mg, po)', '013_Lorazepam (5 mg, po)',\n",
    "#                '014_Fluoxetine project (20 mg, po)', '015_KT alkaloid (60 mg, po)', '016_KT alkaloid (80 mg, po)',\n",
    "#                '017_KT alkaloid (212 mg, cont equal to 10 mg per kg mitragynine, po)',\n",
    "#                '018_Kratom (water extract) (cont equal to 10 mg per kg mitragynine, po)',\n",
    "#                '019_Kratom Syrup (contained 10 mg mitragynine, po)', '020_Haloperidol (0_5 mg, po)',\n",
    "#                '021_Haloperidol (1 mg, po)', '022_Haloperidol+Saline', '023_Haloperidol+5mg METH',\n",
    "#                '024_Morphine II (15 mg, ip)+Naloxone (20 mg, ip)', '025_Jasmine project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f6e9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = final_output, np.hstack(([0]*10, [1]*10, [2]*10, [3]*8, [4]*9, [5]*8, [6]*7, [7]*12, [8]*7, [9]*12, [10]*12, [11]*7,\n",
    "#                                 [12]*10, [13]*7, [14]*9, [15]*7, [16]*8, [17]*10, [18]*9, [19]*8, [20]*9, [21]*7, [22]*7,\n",
    "#                                 [23]*10, [24]*10, [25]*9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3dff50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_folders = ['00_PEG+Saline (control)', \n",
    "#                '02_PEG+5mg METH',\n",
    "#                '04_Morphine II (15 mg, ip)',\n",
    "#                '05_Cannabis project (THC 50 mg, ip)',\n",
    "#                '06_MDMA project (10 mg, ip)',\n",
    "#                '07_L-DOPA project (25 mg in dw, po)', \n",
    "#                '011_Ketamine project (50 mg, ip)', \n",
    "#                '013_Lorazepam (5 mg, po)',\n",
    "#                '014_Fluoxetine project (20 mg, po)',\n",
    "#                '015_KT alkaloid (60 mg, po)']\n",
    "# #                '019_Kratom Syrup (contained 10 mg mitragynine, po)',\n",
    "# #                '025_Jasmine project']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1137b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folders = ['00_Control group',\n",
    "               '04_Morphine II (15 mg, ip)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "78a53c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "../LFP_Bank/00_Control group/2016-08-23--B10--Baseline_200Hz.npy\n",
      "../LFP_Bank/00_Control group/2016-08-24--B2--Baseline_200Hz.npy\n",
      "../LFP_Bank/00_Control group/2016-08-24--B3--Baseline_200Hz.npy\n",
      "../LFP_Bank/00_Control group/2016-08-23--B6--Baseline_200Hz.npy\n",
      "../LFP_Bank/00_Control group/170759--B6--[Saline]_200Hz.npy\n",
      "../LFP_Bank/00_Control group/070759--B2--[Saline]_200Hz.npy\n",
      "../LFP_Bank/00_Control group/080759--B4--[Saline]_200Hz.npy\n",
      "../LFP_Bank/00_Control group/120759--B8--[Saline]_200Hz.npy\n",
      "../LFP_Bank/00_Control group/150759--B10--[Saline]_200Hz.npy\n",
      "../LFP_Bank/00_Control group/160759--B12--[Saline]_200Hz.npy\n",
      "1\n",
      "../LFP_Bank/04_Morphine II (15 mg, ip)/090158--N12--[1R4Mor15]_200Hz.npy\n",
      "../LFP_Bank/04_Morphine II (15 mg, ip)/210258--TN1--[4R5Mor15]_200Hz.npy\n",
      "../LFP_Bank/04_Morphine II (15 mg, ip)/300158--N9--[2R4Mor15]_200Hz.npy\n",
      "../LFP_Bank/04_Morphine II (15 mg, ip)/131157--N15--[1R4Mor15]_200Hz.npy\n",
      "../LFP_Bank/04_Morphine II (15 mg, ip)/271157--N13--[2R4Mor15]_200Hz.npy\n",
      "../LFP_Bank/04_Morphine II (15 mg, ip)/181257--N14--[4R4Mor15]_200Hz.npy\n",
      "../LFP_Bank/04_Morphine II (15 mg, ip)/160158--N11--[2R4Mor15]_200Hz.npy\n",
      "../LFP_Bank/04_Morphine II (15 mg, ip)/210258--TN3--[2R4Mor15]_200Hz.npy\n",
      "../LFP_Bank/04_Morphine II (15 mg, ip)/230158--N10--[3R4Mor15]_200Hz.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30701/2047183342.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  final_output = np.concatenate(np.array(all_segmented_data), axis = 0) # concatenate only axis 0\n",
      "/tmp/ipykernel_30701/2047183342.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  final_label = np.concatenate(np.array(all_label), axis = 0) # concatenate only axis 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19, 450)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_segmented_data = []\n",
    "all_label = []\n",
    "for i, fname in enumerate(all_folders):\n",
    "    print (i)\n",
    "    X_, y_ = segment_data(Folder_name=fname, label=i)\n",
    "    all_segmented_data.append(X_)\n",
    "    all_label.append(y_)\n",
    "final_output = np.concatenate(np.array(all_segmented_data), axis = 0) # concatenate only axis 0\n",
    "final_label = np.concatenate(np.array(all_label), axis = 0) # concatenate only axis 0\n",
    "final_output.shape\n",
    "final_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1bf17c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = final_output, final_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0527bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.zeros((X.shape[0], X.shape[1], D, X.shape[2], X.shape[3]))\n",
    "for i in range (X.shape[0]):\n",
    "#     print(i)\n",
    "    for j in range (X.shape[1]):\n",
    "#         print(j)\n",
    "        delta = butter_bandpass_filter(data=X[i, j], lowcut=1, highcut=4, fs=FS, order=5)\n",
    "        theta = butter_bandpass_filter(data=X[i, j], lowcut=4, highcut=9, fs=FS, order=5)\n",
    "        alpha = butter_bandpass_filter(data=X[i, j], lowcut=9, highcut=13, fs=FS, order=5)\n",
    "        beta = butter_bandpass_filter(data=X[i, j], lowcut=13, highcut=30, fs=FS, order=5)\n",
    "        gamma_I = butter_bandpass_filter(data=X[i, j], lowcut=30, highcut=45, fs=FS, order=5)\n",
    "        gamma_II = butter_bandpass_filter(data=X[i, j], lowcut=60, highcut=95, fs=FS, order=5)\n",
    "        X_new[i, j] = np.array([delta, theta, alpha, beta, gamma_I, gamma_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "90fc666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save test data out\n",
    "np.save('./data/X_meth_control.npy', X_new, allow_pickle=True)\n",
    "np.save('./data/y_meth_control.npy', y, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "95a11c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_values(history):\n",
    "    print(history.history.keys())\n",
    "    val_classifier_accuracy = history.history['val_classifier_accuracy']\n",
    "    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    decoder_loss = history.history['decoder_loss']\n",
    "    val_decoder_loss = history.history['val_decoder_loss']\n",
    "    \n",
    "    classifier_loss = history.history['classifier_loss']\n",
    "    val_classifier_loss = history.history['val_classifier_loss']\n",
    "    return val_classifier_accuracy, loss, val_loss, decoder_loss, val_decoder_loss, classifier_loss, val_classifier_loss\n",
    "\n",
    "def plot_loss(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['val_classifier_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Number of traning iteration')\n",
    "    plt.legend(['Classifier accuracy'], loc = 'lower right')\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Total loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Number of traning iteration')\n",
    "    plt.legend(['Train', 'Validation'], loc = 'upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['decoder_loss'])\n",
    "    plt.plot(history.history['val_decoder_loss'])\n",
    "    plt.title('Mean squared error loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Number of traning iteration')\n",
    "    plt.legend(['Decoder_loss', 'Val_decoder_loss'], loc = 'upper right')\n",
    "    plt.show() \n",
    "    \n",
    "    plt.plot(history.history['classifier_loss'])\n",
    "    plt.plot(history.history['val_classifier_loss'])\n",
    "    plt.title('Cross-entropy loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Number of traning iteration')\n",
    "    plt.legend(['Classifier_loss', 'Val_classifier_loss'], loc = 'upper right')\n",
    "    plt.show() \n",
    "    \n",
    "def plot_tsne(decomposed_data, y_test):\n",
    "    f, ax = plt.subplots(figsize = (10,8))\n",
    "    steps=1\n",
    "    for label in np.unique(y_test):\n",
    "        if label == 0:\n",
    "            text_label = \"Con\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='black', \n",
    "                       linewidths=0.4, color='gray', marker='o', label = str(text_label))\n",
    "        if label == 2:\n",
    "            text_label = \"Meth\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='firebrick',\n",
    "                       linewidths=0.4, color='red', marker='o', label = str(text_label))\n",
    "        if label == 1:\n",
    "            text_label = \"Mor\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='sienna',\n",
    "                       linewidths=0.4, color='sandybrown', marker='o', label = str(text_label))\n",
    "        if label == 3:\n",
    "            text_label = \"THC\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='tan',\n",
    "                       linewidths=0.4, color='moccasin', marker='o', label = str(text_label))\n",
    "        if label == 4:\n",
    "            text_label = \"MDMA\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='olivedrab',\n",
    "                       linewidths=0.4, color='chartreuse', marker='o', label = str(text_label))\n",
    "        if label == 5:\n",
    "            text_label = \"L-DOPA\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='seagreen',\n",
    "                       linewidths=0.4, color='mediumspringgreen', marker='o', label = str(text_label))\n",
    "        if label == 6:\n",
    "            text_label = \"Keta\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='darkcyan',\n",
    "                       linewidths=0.4, color='darkturquoise', marker='o', label = str(text_label))\n",
    "        if label == 7:\n",
    "            text_label = \"Lora\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='navy', \n",
    "                       linewidths=0.4, color='blue', marker='o', label = str(text_label))\n",
    "        if label == 8:\n",
    "            text_label = \"Fluox\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='darkorchid',\n",
    "                       linewidths=0.4, color='plum', marker='o', label = str(text_label))\n",
    "        if label == 9:\n",
    "            text_label = \"AlkaKT\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='mediumvioletred',\n",
    "                       linewidths=0.4, color='palevioletred', marker='o', label = str(text_label))\n",
    "        if label == 10:\n",
    "            text_label = \"SyraKT\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='teal',\n",
    "                       linewidths=0.4, color='cyan', marker='o', label = str(text_label))\n",
    "        if label == 11:\n",
    "            text_label = \"Jas\"\n",
    "            decomposed_class = decomposed_data[label == y_test]\n",
    "            ax.scatter(decomposed_class[::steps, 1], decomposed_class[::steps,0], s = 200, edgecolors='indigo',\n",
    "                       linewidths=0.4, color='lightpink', marker='o', label = str(text_label))\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "49c24342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set channels last system\n",
    "K.set_image_data_format('channels_last')\n",
    "log_path=\"logs_3\"\n",
    "model_name=\"AE\"\n",
    "if not os.path.exists(log_path):\n",
    "    os.makedirs(log_path)\n",
    "\n",
    "# Set folder for saving model\n",
    "model_path = \"model\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f0780a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(X, y, epochs, batch_size):\n",
    "    \n",
    "    X = X.reshape(-1,D,C,T)\n",
    "    X = np.swapaxes(X, 2, 3)\n",
    "    \n",
    "    yl = y.reshape(-1)\n",
    "    \n",
    "    ys = np.arange(y.shape[0])\n",
    "    ys = np.repeat([ys], y.shape[1], axis=0)\n",
    "    ys = np.swapaxes(ys, 0, 1)\n",
    "    ys = ys.reshape(-1)\n",
    "            \n",
    "    skf = StratifiedKFold(n_splits = 2, random_state = 42, shuffle = True)\n",
    "    skf.get_n_splits(X, ys)\n",
    "    print(skf)\n",
    "    \n",
    "    fold=0\n",
    "    y_true_all, y_pred_all, scores_all, clas_report_all  = [], [], [], []\n",
    "    val_clas_acc_all, loss_all, val_loss_all, dec_loss_all = [], [], [], []\n",
    "    val_dec_loss_all, clas_loss_all, val_clas_loss_all = [], [], []\n",
    "    \n",
    "    for learn_index, test_index in skf.split(X, ys):\n",
    "        print(\"LEARN:\", learn_index, \"TEST:\", test_index)\n",
    "        X_learn, X_test = X[learn_index], X[test_index]\n",
    "#         print('X_learn:', X_learn)\n",
    "        y_learn, y_test = yl[learn_index], yl[test_index]\n",
    "#         print('y_learn:', y_learn)\n",
    "        ys_learn, ys_test = ys[learn_index], ys[test_index]\n",
    "#         print('ys_learn:', ys_learn)\n",
    "\n",
    "        skf_train = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "        skf_train.get_n_splits(X_learn, ys_learn)\n",
    "        print(skf_train)\n",
    "        \n",
    "        y_true_fold, y_pred_fold, scores_fold, clas_report_fold = [], [], [], []\n",
    "        val_clas_acc_fold, loss_fold, val_loss_fold, dec_loss_fold = [], [], [], []\n",
    "        val_dec_loss_fold, clas_loss_fold, val_clas_loss_fold = [], [], []\n",
    "        \n",
    "        for train_index, val_index in skf_train.split(X_learn, ys_learn):\n",
    "            print(\"TRAIN:\", train_index, \"VAL:\", val_index)\n",
    "            X_train, X_val = X_learn[train_index], X_learn[val_index]\n",
    "#             print('X_train:', X_train)\n",
    "#             print('X_val:', X_val)\n",
    "            y_train, y_val = y_learn[train_index], y_learn[val_index]\n",
    "#             print('y_train:', y_train)\n",
    "            ys_train, ys_val = ys_learn[train_index], ys_learn[val_index]\n",
    "#             print('ys_train:', ys_train)\n",
    "            \n",
    "            y_train_dummy = np.zeros_like(y_train)\n",
    "            y_val_dummy = np.zeros_like(y_val)\n",
    "            y_test_dummy = np.zeros_like(y_test)\n",
    "            \n",
    "            fold += 1\n",
    "            \n",
    "            # reset model\n",
    "            weights_dir = log_path+'/'+model_name+'_out_weights'+str(fold)+'.h5'\n",
    "            num_class = len(np.unique(y))\n",
    "            model = build(num_class = num_class)\n",
    "            model.summary()\n",
    "            optimizer = Adam(learning_rate = 1e-4, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08)\n",
    "            model.compile(optimizer = optimizer, \n",
    "                          loss = ['mean_squared_error', 'sparse_categorical_crossentropy'],\n",
    "                          metrics = ['accuracy'], loss_weights = [1., 1.])\n",
    "              \n",
    "            # set callbacks\n",
    "            checkpointer  = ModelCheckpoint(monitor='val_loss',\n",
    "                                            filepath=log_path+'/'+model_name+'_out_weights'+str(fold)+'.h5', \n",
    "                                            verbose=1, save_best_only=True, save_weight_only=True)\n",
    "            csv_logger    = CSVLogger(log_path+'/'+model_name+'_out_log'+str(fold)+'.log')\n",
    "            reduce_lr     = ReduceLROnPlateau(monitor='val_loss', patience=5, factor=0.5, mode='min',\n",
    "                                              verbose=1, min_lr=1e-5)\n",
    "            es            = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20) \n",
    "                            # patience = save data at n ecoch followed by 10 bad epoch and stop \n",
    "            \n",
    "            # fit model\n",
    "            history = model.fit(x = X_train, y = [X_train, y_train], \n",
    "                            epochs = epochs, # n of cycle for training \n",
    "                            shuffle = True, \n",
    "                            batch_size = batch_size, \n",
    "                            validation_data = (X_val, [X_val, y_val]),\n",
    "                            callbacks=[checkpointer, csv_logger, reduce_lr, es])\n",
    "            \n",
    "            val_clas_acc, loss, val_loss, dec_loss, val_dec_loss, clas_loss, val_clas_loss = history_values(history)\n",
    "            \n",
    "            val_clas_acc_fold.append(val_clas_acc)\n",
    "            loss_fold.append(loss)\n",
    "            val_loss_fold.append(val_loss)\n",
    "            dec_loss_fold.append(dec_loss)\n",
    "            val_dec_loss_fold.append(val_dec_loss)\n",
    "            clas_loss_fold.append(clas_loss)\n",
    "            val_clas_loss_fold.append(val_clas_loss)\n",
    "            \n",
    "            def build_test():\n",
    "                model = build(num_class = num_class)\n",
    "                encoder_input = model.layers[0].output\n",
    "                encoder = model.layers[1]\n",
    "                decoder = model.layers[2]\n",
    "                classifier = model.layers[3]\n",
    "                latent = encoder(encoder_input)\n",
    "                train_xr = decoder(latent)\n",
    "                z = classifier(latent)\n",
    "                model = models.Model(inputs = [encoder_input], outputs = [latent, train_xr, z],  name = 'AE')\n",
    "                model.compile(optimizer = optimizer, \n",
    "                              loss = [triplet_loss(margin=1.), 'mean_squared_error', 'sparse_categorical_crossentropy'],\n",
    "                              metrics = ['accuracy'], loss_weights = [0., 1., 1.])\n",
    "                return model\n",
    "            \n",
    "            model = build_test()\n",
    "            model.load_weights(weights_dir)\n",
    "            \n",
    "            # saving model\n",
    "            model.save(f'{model_path}/00_6_autoencoder_[fit]_balanced_class-6-bands-wo-loss-test_2.h5')\n",
    "            \n",
    "            latent, train_xr, z = model.predict(X_test, batch_size = batch_size)\n",
    "            \n",
    "            y_true_fold.append(y_test)\n",
    "            y_pred = np.argmax(z, axis=1)\n",
    "            y_pred_fold.append(y_pred)\n",
    "            \n",
    "            print(model.evaluate(x = X_test, y = [y_test_dummy, X_test, y_test]))\n",
    "            print(classification_report(y_test, y_pred, output_dict=True))\n",
    "            \n",
    "            scores = model.evaluate(x = X_test, y = [y_test_dummy, X_test, y_test])\n",
    "            clas_report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        \n",
    "            scores_fold.append(scores)\n",
    "            clas_report_fold.append(clas_report)\n",
    "                           \n",
    "            tsne = TSNE(n_components = 2, random_state = 42)\n",
    "            decomposed_data = tsne.fit_transform(latent)\n",
    "            \n",
    "            # plot loss\n",
    "            plot_loss(history)\n",
    "            plot_tsne(decomposed_data, y_test)\n",
    "            \n",
    "            K.clear_session()\n",
    "            \n",
    "        val_clas_acc_all.append(val_clas_acc_fold)\n",
    "        val_clas_acc_all_data = np.array(val_clas_acc_all)\n",
    "        loss_all.append(loss_fold)\n",
    "        loss_all_data = np.array(loss_all)\n",
    "        val_loss_all.append(val_loss_fold)\n",
    "        val_loss_all_data = np.array(val_loss_all)\n",
    "        dec_loss_all.append(dec_loss_fold)\n",
    "        dec_loss_all_data = np.array(dec_loss_all)\n",
    "        val_dec_loss_all.append(val_dec_loss_fold)\n",
    "        val_dec_loss_all_data = np.array(val_dec_loss_all)\n",
    "        clas_loss_all.append(clas_loss_fold)\n",
    "        clas_loss_all_data = np.array(clas_loss_all)\n",
    "        val_clas_loss_all.append(val_clas_loss_fold)\n",
    "        val_clas_loss_all_data = np.array(val_clas_loss_all)\n",
    "        \n",
    "        y_true_all.append(y_true_fold)\n",
    "        y_true_all_data = np.array(y_true_all)\n",
    "        \n",
    "        y_pred_all.append(y_pred_fold)\n",
    "        y_pred_all_data = np.array(y_pred_all)\n",
    "        \n",
    "        scores_all.append(scores_fold)\n",
    "        scores_all_data = np.array(scores_all)\n",
    "        \n",
    "        clas_report_all.append(clas_report_fold)\n",
    "        clas_report_all_data = np.array(clas_report_all)\n",
    "    \n",
    "    return (val_clas_acc_all_data, loss_all_data, val_loss_all_data, dec_loss_all_data, val_dec_loss_all_data,\n",
    "            clas_loss_all_data, val_clas_loss_all_data, y_true_all_data, y_pred_all_data, scores_all_data,\n",
    "            clas_report_all_data, train_xr, z, fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22339562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=2, random_state=42, shuffle=True)\n",
      "LEARN: [   0    1    4 ... 8542 8543 8547] TEST: [   2    3    5 ... 8546 8548 8549]\n",
      "StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
      "TRAIN: [   1    2    3 ... 4272 4273 4274] VAL: [   0   11   13   14   21   32   55   56   60   62   63   67   76   77\n",
      "   78   79   80   88   93   97   98  110  113  116  117  120  121  126\n",
      "  128  137  139  141  145  150  153  162  163  170  171  175  193  200\n",
      "  203  217  221  226  227  244  248  250  253  254  265  282  284  287\n",
      "  288  289  292  293  294  298  302  306  313  323  324  328  339  343\n",
      "  349  350  353  354  358  360  363  364  367  368  379  381  409  411\n",
      "  415  421  423  428  432  447  471  479  481  484  493  496  500  503\n",
      "  510  512  515  519  524  528  531  537  541  542  545  555  565  567\n",
      "  575  583  584  588  591  593  597  608  611  618  619  625  638  640\n",
      "  641  642  644  650  653  654  656  658  669  678  683  684  690  692\n",
      "  707  710  712  713  714  719  721  725  738  740  749  751  753  754\n",
      "  762  766  767  773  784  788  809  819  824  828  829  831  833  841\n",
      "  842  846  855  859  861  866  868  870  872  880  891  893  909  911\n",
      "  914  937  939  943  944  948  952  953  972  974  980  991  992  994\n",
      "  997  999 1002 1004 1015 1017 1020 1025 1029 1040 1041 1048 1052 1053\n",
      " 1055 1060 1061 1065 1069 1070 1076 1094 1095 1096 1107 1111 1114 1122\n",
      " 1124 1125 1133 1138 1139 1141 1145 1147 1150 1152 1155 1158 1163 1164\n",
      " 1167 1182 1184 1188 1190 1193 1195 1199 1203 1206 1210 1211 1218 1221\n",
      " 1223 1230 1249 1257 1262 1271 1272 1274 1277 1298 1307 1312 1315 1317\n",
      " 1343 1345 1347 1348 1352 1354 1355 1357 1361 1364 1368 1384 1385 1386\n",
      " 1390 1397 1405 1409 1433 1448 1451 1452 1461 1465 1467 1474 1475 1476\n",
      " 1478 1489 1492 1497 1498 1499 1512 1523 1525 1526 1533 1535 1536 1541\n",
      " 1551 1560 1565 1566 1568 1569 1571 1577 1579 1584 1591 1592 1606 1614\n",
      " 1615 1620 1624 1627 1636 1644 1646 1647 1649 1651 1655 1656 1659 1665\n",
      " 1670 1681 1687 1692 1697 1702 1706 1707 1716 1727 1733 1735 1739 1744\n",
      " 1745 1749 1757 1767 1768 1779 1780 1785 1789 1796 1802 1805 1809 1811\n",
      " 1812 1829 1835 1862 1866 1870 1871 1872 1874 1881 1884 1885 1894 1897\n",
      " 1906 1909 1910 1911 1912 1919 1920 1922 1923 1933 1936 1938 1945 1951\n",
      " 1955 1957 1965 1966 1970 1971 1975 1978 1983 1992 1994 1998 1999 2026\n",
      " 2028 2039 2041 2047 2049 2058 2060 2088 2089 2095 2098 2108 2114 2116\n",
      " 2120 2123 2124 2125 2127 2128 2130 2131 2132 2145 2146 2154 2160 2161\n",
      " 2162 2163 2166 2172 2173 2177 2179 2196 2199 2209 2223 2225 2226 2235\n",
      " 2245 2248 2254 2255 2261 2265 2266 2271 2280 2284 2285 2287 2292 2299\n",
      " 2303 2307 2312 2316 2321 2338 2347 2351 2352 2354 2362 2363 2365 2367\n",
      " 2377 2379 2385 2396 2397 2411 2419 2427 2434 2436 2438 2443 2446 2450\n",
      " 2453 2455 2460 2471 2474 2481 2486 2488 2492 2496 2500 2503 2504 2508\n",
      " 2519 2521 2526 2540 2542 2545 2551 2560 2561 2569 2570 2577 2580 2581\n",
      " 2586 2589 2590 2592 2594 2598 2600 2602 2604 2614 2622 2629 2646 2661\n",
      " 2663 2667 2669 2677 2683 2684 2691 2693 2701 2703 2704 2710 2712 2713\n",
      " 2716 2721 2728 2729 2733 2736 2742 2743 2744 2748 2757 2759 2768 2771\n",
      " 2772 2779 2780 2783 2786 2793 2799 2801 2806 2815 2819 2826 2832 2838\n",
      " 2845 2872 2884 2889 2905 2911 2912 2916 2917 2921 2923 2926 2929 2934\n",
      " 2941 2943 2949 2950 2956 2965 2970 2978 2988 2995 2996 2999 3008 3011\n",
      " 3012 3022 3024 3026 3032 3054 3059 3066 3067 3072 3078 3079 3081 3090\n",
      " 3093 3094 3098 3112 3114 3116 3117 3124 3127 3128 3130 3137 3138 3143\n",
      " 3151 3153 3158 3159 3161 3162 3165 3169 3172 3175 3189 3194 3199 3201\n",
      " 3215 3219 3223 3225 3229 3237 3243 3246 3255 3258 3270 3275 3290 3295\n",
      " 3298 3305 3306 3312 3314 3318 3321 3328 3337 3343 3344 3345 3350 3359\n",
      " 3360 3363 3374 3387 3390 3394 3398 3399 3407 3411 3414 3416 3419 3430\n",
      " 3438 3439 3442 3447 3454 3460 3468 3470 3471 3472 3480 3485 3487 3490\n",
      " 3494 3495 3496 3505 3515 3520 3522 3525 3528 3531 3535 3541 3542 3552\n",
      " 3560 3570 3578 3582 3594 3596 3600 3603 3605 3610 3640 3642 3648 3664\n",
      " 3673 3682 3685 3695 3704 3705 3706 3708 3710 3711 3717 3720 3721 3736\n",
      " 3737 3740 3741 3745 3749 3751 3754 3755 3757 3760 3768 3773 3777 3787\n",
      " 3792 3797 3803 3806 3807 3808 3812 3822 3824 3826 3831 3837 3839 3845\n",
      " 3859 3866 3867 3868 3874 3876 3878 3881 3883 3885 3889 3899 3900 3910\n",
      " 3919 3924 3927 3936 3939 3942 3945 3954 3966 3970 3971 3976 3977 3978\n",
      " 3988 3998 3999 4006 4011 4014 4024 4029 4039 4040 4043 4044 4060 4061\n",
      " 4065 4068 4071 4075 4076 4080 4084 4090 4095 4096 4098 4099 4100 4102\n",
      " 4113 4119 4121 4134 4143 4147 4149 4152 4154 4155 4159 4163 4164 4173\n",
      " 4176 4180 4185 4202 4204 4208 4209 4212 4228 4232 4239 4250 4256 4259\n",
      " 4266]\n",
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 6, 800, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 6, 800, 16)        496       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 6, 800, 16)        64        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 6, 80, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 6, 80, 8)          1288      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 6, 80, 8)          32        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_9 (Average (None, 6, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 200)               77000     \n",
      "=================================================================\n",
      "Total params: 78,880\n",
      "Trainable params: 78,832\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 384)               77184     \n",
      "_________________________________________________________________\n",
      "reshape_4 (Reshape)          (None, 6, 8, 8)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 6, 8, 8)           648       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 6, 80, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 6, 80, 16)         1296      \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, 6, 800, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 6, 800, 3)         483       \n",
      "=================================================================\n",
      "Total params: 79,611\n",
      "Trainable params: 79,611\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"AE\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 6, 800, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 (None, 200)          78880       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 (None, 6, 800, 3)    79611       encoder[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "classifier (Dense)              (None, 2)            402         encoder[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 158,893\n",
      "Trainable params: 158,845\n",
      "Non-trainable params: 48\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/71\n",
      "26/27 [===========================>..] - ETA: 0s - loss: 0.6712 - decoder_loss: 0.0209 - classifier_loss: 0.6503 - decoder_accuracy: 0.3292 - classifier_accuracy: 0.6430"
     ]
    }
   ],
   "source": [
    "(val_clas_acc, loss, val_loss, dec_loss, val_dec_loss, clas_loss, val_clas_loss, y_true_all_data, y_pred_all_data, \n",
    " scores_all_data, clas_report_all_data, train_xr, z, fold) = fit_model(X = X_new, y = y, epochs = 71, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report classifier_accuracy\n",
    "val_classifier_accuracy = scores_all_data[:,:,-1].reshape(-1)*100\n",
    "mean_classifier_accuracy = mean(val_classifier_accuracy) \n",
    "sem_classifier_accuracy = sem(val_classifier_accuracy)\n",
    " \n",
    "print(\"Mean:\", mean_classifier_accuracy, \"SEM:\", sem_classifier_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b49baac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export classification_report\n",
    "# xx = []\n",
    "# for i in range (fold):\n",
    "#     a = clas_report_all_data.reshape(-1)[i].get('macro avg')\n",
    "#     x = a.get(\"f1-score\")\n",
    "#     xx.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_clas_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed48f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss_mean(val_clas_acc, loss, val_loss, dec_loss, val_dec_loss, clas_loss, val_clas_loss):\n",
    "    val_clas_accu = val_clas_acc.reshape(-1, val_clas_acc.shape[2])\n",
    "    val_clas_accu = np.swapaxes(val_clas_accu, 0, 1)\n",
    "    val_clas_aver = np.average(val_clas_accu, axis=1)*100\n",
    "    val_clas_sem = stats.sem(val_clas_accu, axis=1)*100\n",
    "    \n",
    "    losss = loss.reshape(-1, loss.shape[2])\n",
    "    losss = np.swapaxes(losss, 0, 1)\n",
    "    loss_aver = np.average(losss, axis=1)\n",
    "    loss_sem = stats.sem(losss, axis=1)\n",
    "    \n",
    "    val_losss = val_loss.reshape(-1, val_loss.shape[2])\n",
    "    val_losss = np.swapaxes(val_losss, 0, 1)\n",
    "    val_loss_aver = np.average(val_losss, axis=1)\n",
    "    val_loss_sem = stats.sem(val_losss, axis=1)\n",
    "    \n",
    "    dec_losss = dec_loss.reshape(-1, dec_loss.shape[2])\n",
    "    dec_losss = np.swapaxes(dec_losss, 0, 1)\n",
    "    dec_loss_aver = np.average(dec_losss, axis=1)\n",
    "    dec_loss_sem = stats.sem(dec_losss, axis=1)\n",
    "    \n",
    "    val_dec_losss = val_dec_loss.reshape(-1, val_dec_loss.shape[2])\n",
    "    val_dec_losss = np.swapaxes(val_dec_losss, 0, 1)\n",
    "    val_dec_loss_aver = np.average(val_dec_losss, axis=1)\n",
    "    val_dec_loss_sem = stats.sem(val_dec_losss, axis=1)\n",
    "    \n",
    "    clas_losss = clas_loss.reshape(-1, clas_loss.shape[2])\n",
    "    clas_losss = np.swapaxes(clas_losss, 0, 1)\n",
    "    clas_loss_aver = np.average(clas_losss, axis=1)\n",
    "    clas_loss_sem = stats.sem(clas_losss, axis=1)\n",
    "    \n",
    "    val_clas_losss = val_clas_loss.reshape(-1, val_clas_loss.shape[2])\n",
    "    val_clas_losss = np.swapaxes(val_clas_losss, 0, 1)\n",
    "    val_clas_loss_aver = np.average(val_clas_losss, axis=1)\n",
    "    val_clas_loss_sem = stats.sem(val_clas_losss, axis=1)\n",
    "\n",
    "    \n",
    "    sns.set()\n",
    "    # x = np.arange(len(val_classifier_average))\n",
    "    x = np.arange(71) # limit x axis\n",
    "    plt.figure(figsize=(10, 7.3))\n",
    "    plt.plot(x, val_clas_aver, 'b-', label='Classifier accuracy')\n",
    "    plt.fill_between(x, val_clas_aver - val_clas_sem, val_clas_aver + val_clas_sem, color='b', alpha=0.2)\n",
    "    plt.title('Model accuracy', fontdict=dict(weight='bold'))\n",
    "    plt.ylabel('Accuracy', fontdict=dict(weight='bold'))\n",
    "    plt.xlabel('Number of traning iteration', fontdict=dict(weight='bold'))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 7.3))\n",
    "    plt.plot(x, loss_aver, 'b-', label='Training')\n",
    "    plt.fill_between(x, loss_aver - loss_sem, loss_aver + loss_sem, color='b', alpha=0.2)\n",
    "    plt.plot(x, val_loss_aver, 'r--', label='Validation')\n",
    "    plt.fill_between(x, val_loss_aver - val_loss_sem, val_loss_aver + val_loss_sem, color='r', alpha=0.2)\n",
    "    plt.title('Total loss', fontdict=dict(weight='bold'))\n",
    "    plt.ylabel('Loss', fontdict=dict(weight='bold'))\n",
    "    plt.xlabel('Number of traning iteration', fontdict=dict(weight='bold'))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 7.3))\n",
    "    plt.plot(x, dec_loss_aver, 'b-', label='Training')\n",
    "    plt.fill_between(x, dec_loss_aver - dec_loss_sem, dec_loss_aver + dec_loss_sem, color='b', alpha=0.2)\n",
    "    plt.plot(x, val_dec_loss_aver, 'r--', label='Validation')\n",
    "    plt.fill_between(x, val_dec_loss_aver - val_dec_loss_sem, val_dec_loss_aver + val_dec_loss_sem, color='r', alpha=0.2)\n",
    "    plt.title('Mean square error loss', fontdict=dict(weight='bold'))\n",
    "    plt.ylabel('Loss', fontdict=dict(weight='bold'))\n",
    "    plt.xlabel('Number of traning iteration', fontdict=dict(weight='bold'))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 7.3))\n",
    "    plt.plot(x, clas_loss_aver, 'b-', label='Training')\n",
    "    plt.fill_between(x, clas_loss_aver - clas_loss_sem, clas_loss_aver + clas_loss_sem, color='b', alpha=0.2)\n",
    "    plt.plot(x, val_clas_loss_aver, 'r--', label='Validation')\n",
    "    plt.fill_between(x, val_clas_loss_aver - val_clas_loss_sem, val_clas_loss_aver + val_clas_loss_sem, color='r', alpha=0.2)\n",
    "    plt.title('Cross-entropy loss', fontdict=dict(weight='bold'))\n",
    "    plt.ylabel('Loss', fontdict=dict(weight='bold'))\n",
    "    plt.xlabel('Number of traning iteration', fontdict=dict(weight='bold'))\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b8932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_loss_mean(val_clas_acc, loss, val_loss, dec_loss, val_dec_loss, clas_loss, val_clas_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb8b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_true_all_data.reshape(-1)\n",
    "y_pred = y_pred_all_data.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a998c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = []\n",
    "# for i in range(fold):\n",
    "#     for y in y_true_all_data.reshape(-1)[i]:\n",
    "#         y_true.append(y)\n",
    "#         y_true_all = np.array(y_true)\n",
    "        \n",
    "# y_pred = []import matplotlib.pyplot as plt\n",
    "# for i in range(fold):\n",
    "#     for y in y_pred_all_data.reshape(-1)[i]:\n",
    "#         y_pred.append(y)\n",
    "#         y_pred_all = np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d55a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_data = y_true\n",
    "predicted_data = y_pred\n",
    "cm = confusion_matrix(actual_data, predicted_data)\n",
    "# print(cm)\n",
    "plt.figure(figsize=(10, 7.3))\n",
    "ax = sns.heatmap(cm, annot=True, fmt='g',cmap=\"Blues_r\",annot_kws={'size':9});\n",
    "ax.set_title('Confusion Matrix', fontdict=dict(weight='bold'));\n",
    "ax.set_xlabel('Predicted class', fontdict=dict(weight='bold'))\n",
    "ax.set_ylabel('Actual class', fontdict=dict(weight='bold'));\n",
    "ax.yaxis.set_ticklabels(['Con', 'Mor'])\n",
    "ax.xaxis.set_ticklabels(['Con', 'Mor'])\n",
    "# ax.yaxis.set_ticklabels(['Con', 'Meth', 'Mor','THC','MDMA','L-DOPA', 'Keta','Lora','Fluox','AlkaKT'])\n",
    "# ax.xaxis.set_ticklabels(['Con', 'Meth', 'Mor','THC','MDMA','L-DOPA', 'Keta','Lora','Fluox','AlkaKT'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdbdd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)\n",
    "# print(cm)\n",
    "plt.figure(figsize=(10, 7.3))\n",
    "ax = sns.heatmap(cm/np.sum(cm), annot=True, fmt='.2%', cmap=\"Blues_r\",annot_kws={'size':9})\n",
    "ax.set_title('Confusion Matrix', fontdict=dict(weight='bold'));\n",
    "ax.set_xlabel('Predicted class', fontdict=dict(weight='bold'))\n",
    "ax.set_ylabel('Actual class', fontdict=dict(weight='bold'));\n",
    "ax.yaxis.set_ticklabels(['Con', 'Mor'])\n",
    "ax.xaxis.set_ticklabels(['Con', 'Mor'])\n",
    "# ax.yaxis.set_ticklabels(['Con', 'Meth', 'Mor','THC','MDMA','L-DOPA', 'Keta','Lora','Fluox','AlkaKT'])\n",
    "# ax.xaxis.set_ticklabels(['Con', 'Meth', 'Mor','THC','MDMA','L-DOPA', 'Keta','Lora','Fluox','AlkaKT'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0e0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = np.array([ y  for y in y_true_all_data.reshape(-1)[i] for i in range(45)])\n",
    "# P = np.array([ y  for y in y_pred_all_data.reshape(-1)[i] for i in range(45)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e1e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = []\n",
    "# for i in range(36):\n",
    "#     for y in y_true_all_data.reshape(-1)[i]:\n",
    "#         y_true.append(y)\n",
    "#         y_true_all = np.array(y_true)\n",
    "        \n",
    "# y_pred = []\n",
    "# for i in range(4):\n",
    "#     for y in y_pred_all_data.reshape(-1)[i]:\n",
    "#         y_pred.append(y)\n",
    "#         y_pred_all = np.array(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53def6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "# X = np.array([[0, 0], [1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6], [7, 7],[8, 8], [9, 9], [10, 10], [11, 11]])\n",
    "# y = np.array([0, 0,0,1,1,1, 1,1,1,1,1, 1])\n",
    "# ys = np.array([0,0,0,1,1,1,2,2,2,3,3,3])\n",
    "# skf = StratifiedKFold(n_splits=2)\n",
    "# skf.get_n_splits(X, ys)\n",
    "# print(skf)\n",
    "# StratifiedKFold(n_splits=2, random_state=42, shuffle=True)\n",
    "# for learn_index, test_index in skf.split(X, ys):\n",
    "#     print(\"LEARN:\", learn_index, \"TEST:\", test_index)\n",
    "#     X_learn, X_test = X[learn_index], X[test_index]\n",
    "#     print('X_learn:', X_learn)\n",
    "#     y_learn, y_test = y[learn_index], y[test_index]\n",
    "#     print('y_learn:', y_learn)\n",
    "#     ys_learn, ys_test = ys[learn_index], ys[test_index]\n",
    "#     print('ys_learn:', ys_learn)\n",
    "    \n",
    "#     skf_train = StratifiedKFold(n_splits=2, random_state=42, shuffle=True)\n",
    "#     skf_train.get_n_splits(X_learn, ys_learn)\n",
    "#     print(skf_train)\n",
    "#     for train_index, val_index in skf_train.split(X_learn, ys_learn):\n",
    "#         print(\"TRAIN:\", train_index, \"VAL:\", val_index)\n",
    "#         X_train, X_val = X_learn[train_index], X_learn[val_index]\n",
    "#         print('X_train:', X_train)\n",
    "#         print('X_val:', X_val)\n",
    "#         y_train, y_val = y_learn[train_index], y_learn[val_index]\n",
    "#         print('y_train:', y_train)\n",
    "#         ys_train, ys_val = ys_learn[train_index], ys_learn[val_index]\n",
    "#         print('ys_train:', ys_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
